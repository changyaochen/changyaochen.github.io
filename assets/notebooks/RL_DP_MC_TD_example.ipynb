{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24a63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a41970-66c1-49fe-8af3-0546f0bdfa5d",
   "metadata": {},
   "source": [
    "# Dynamic Programming (DP) vs Monte Carlo (MC) vs Temporal Difference (TD)\n",
    "\n",
    "In this example, the agent starts from state 3, and always takes a 50/50 action between moving left or right. The state 0 and 6 are terminal states, and moving from state 5 to 6 gives the agent a reward of +1. As such, this is an episodic task. We will use 3 different approaches: Dynamic Programming (DP), Monte Carlo (MC), and Temporal Difference to solve for the state values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddf232-5f44-4511-b566-0bef08e5031c",
   "metadata": {},
   "source": [
    "![title](../images/dp_mc_td_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d15fb-6264-451c-9f90-156c8388a387",
   "metadata": {},
   "source": [
    "## DP solution\n",
    "\n",
    "To solve for the state values with DP, we need to specific the state transition probability under the current policy (50/50 random), and the reward. They can be written as:\n",
    "\n",
    "$$\n",
    "p(s' \\mid s) = 0.5 \\times\n",
    "  \\begin{bmatrix}\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "  0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "  0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n",
    "  0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "  \\end{bmatrix} \\\\\n",
    "\n",
    "R =\n",
    "  \\begin{bmatrix}\n",
    "  0 & 0 & 0 & 0 & 0 & 0.5 & 0\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65427635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.16666667, 0.33333333, 0.5       , 0.66666667,\n",
       "       0.83333333, 0.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = 0.5 * np.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "])\n",
    "\n",
    "rewards = np.array([0, 0, 0, 0, 0, 0.5, 0]).T\n",
    "\n",
    "np.linalg.inv(np.eye(N=7) - proba) @ rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fa3e8-d98c-4713-b528-42efb2dc5e62",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4d1f6f-9b02-42ed-b9c3-b68fb534e085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 0.17792421746293247,\n",
       " 2: 0.341688654353562,\n",
       " 3: 0.501,\n",
       " 4: 0.6779431664411367,\n",
       " 5: 0.8549488054607508,\n",
       " 6: 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trials = 1_000\n",
    "\n",
    "state_rewards = {i: [] for i in range(7)}\n",
    "\n",
    "gamma = 1.\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    current_state = 3  # Starting state \n",
    "    step = 0\n",
    "    first_visit = {current_state: 0}\n",
    "    \n",
    "    state_history = [current_state]\n",
    "    reward_history = [0]\n",
    "    \n",
    "    while current_state not in (0, 6):\n",
    "        step += 1\n",
    "        next_state = current_state + np.random.choice([-1, 1])\n",
    "        if not next_state in first_visit:\n",
    "            first_visit[next_state] = step\n",
    "        \n",
    "        state_history.append(next_state)\n",
    "        if next_state == 6:\n",
    "            reward_history.append(1)\n",
    "        else:\n",
    "            reward_history.append(0)\n",
    "        \n",
    "        current_state = next_state \n",
    "        \n",
    "    # Makes updates from the end of the episode\n",
    "    total_reward = 0\n",
    "    for i in range(step, -1, -1):\n",
    "        total_reward = (reward_history[i] + gamma * total_reward)\n",
    "        # Is this the first visit?\n",
    "        if first_visit[state_history[i]] == i:\n",
    "            state_rewards[state_history[i]].append(total_reward)\n",
    "\n",
    "state_values_mc = {k: np.mean(v) for k, v in state_rewards.items()}\n",
    "state_values_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81e7cc-4046-4b45-bf02-b7f07b05b0c9",
   "metadata": {},
   "source": [
    "## TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9050d2bc-3dbc-4cd8-90cb-6b8e83eac37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0.19474867055697664,\n",
       " 2: 0.3474823346423158,\n",
       " 3: 0.46470878605967475,\n",
       " 4: 0.6839359721925832,\n",
       " 5: 0.9176225189141505,\n",
       " 6: 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_values_td = {i: 0 for i in range(7)}\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    current_state = 3  # Starting state \n",
    "    \n",
    "    while current_state not in (0, 6):\n",
    "        next_state = current_state + np.random.choice([-1, 1])\n",
    "        reward = 1 if next_state == 6 else 0\n",
    "        \n",
    "        state_values_td[current_state] += alpha * (reward + gamma * state_values_td[next_state] - state_values_td[current_state])\n",
    "        current_state = next_state\n",
    "        \n",
    "state_values_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7cc3c-d19c-463d-9e95-39451053c13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
