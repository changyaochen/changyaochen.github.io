---
layout: single
title:  "A short note on the Evidence Lower Bound (ELBO)"
date:   2024-10-04 12:00:00 -0600
published: true
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: A short note on the Evidence Lower Bound (ELBO) in variational inference.
header:
  teaser: assets/images/oreilly_gen_ai_book.jpeg
---

This is a short note on my understanding (and the journey) of the Evidence Lower Bound (ELBO) while learning the generative AI.

## Bayesian machine learning

One could argue Bayesian machine learning is at the heart of generative AI. In the Bayesian framework, we are more interested in the **distribution** of the model parameters $$\theta$$, as opposed to the **point estimate** of $$\theta$$ in the frequentist framework. Take simple linear regression as an example, in the frequentist framework, we estimate the values of $$\theta$$ by minimizing the mean squared error (MSE) between the predicted and the true values. In the Bayesian framework, we want to estimate the distribution of the parameters $$\theta$$, in which we start with a prior distribution of $$\theta$$, _e.g._, a Gaussian distribution with unknown mean and variance, and update the distribution using the observed data, usually through maximum likelihood estimation (MLE). The updated distribution of $$\theta$$ is called the **posterior distribution**.

As we can see, the Bayesian framework provides more information, notably, about the uncertainty of the model. This is particularly useful in the context of generative AI, where we want to generate new data points that are similar to the training data $$X$$, which are sampled from a unknown distribution.

## Latent variables

In the context of generative AI, we often introduce **latent variables** to model the data generation process. The latent variables are not observed in the training data, but they are responsible for generating the observed data. For example, in the simple linear regression, we can treat the slope and intercept as the latent variables, albeit not a common convention as they are usually treated as the model **parameters**. Starting from a prior distribution (_e.g._, Gaussian), we then use Bayesian rule to obtain the posterior distribution using the observed data.


Example eye color (latent variable) to image (data)
