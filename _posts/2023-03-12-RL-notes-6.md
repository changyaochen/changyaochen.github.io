---
layout: single
title:  "Notes on Reinforcement Learning: TD for control"
date:   2023-03-12 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---
TD for prediction.

SARSA algorithm. SARSA stands for the $s_{t}, a_{t}, r_{t}, s_{t+1}, a_{t+1}$, it is an acronym for the sequence of the update rule.

$$
q(s_{t}, a_{t}) \leftarrow q(s_{t}, a_{t}) +
\alpha(r_{t} + \gamma q(s_{t+1}, a_{t+1}) - q(s_{t}, a_{t}))
$$

Expected SARSA algorithm. In SARSA, we have to wait first take the next action $a_{t+1}$, and wait for the next state $s_{t+1}$, in order to make update to $q(s_{t}, a_{t})$ based on $q(s_{t+1}, a_{t+1})$. However, since we already know the policy we are following, we can calculate the **expected** $q(s_{t+1}, a_{t+1})$ without waiting for the next state $s_{t+1}$. Therefore, the update rule becomes:

$$
q(s_{t}, a_{t}) \leftarrow q(s_{t}, a_{t}) +
\alpha(r_{t} + \gamma \sum_{a'} \pi(a' \mid s_{t+1}) q(s_{t+1}, a') - q(s_{t}, a_{t}))
$$

It appears that expected SARSA should always be preferred over SARSA, since what we are interested is the long-term, expected, behavior, then taking expectation early on is a good idea (as opposed to from discrete sampling). This mitigates the variance from the behavior policy. However, the expectation can be expensive to calculate if the action space is large.

Q-learning: just a little deviation from SARSA: applying Bellman optimality equation on SARSA:

$$
q(s_{t}, a_{t}) \leftarrow q(s_{t}, a_{t}) +
\alpha(r_{t} + \gamma \max_{a'} q(s_{t+1}, a') - q(s_{t}, a_{t}))
$$

Q-learning gets us the optimal state-action values, not necessarily the policy (although we can greedify to get the optimal policy)

In this way, we mingle the policy evaluation and policy improvement steps. Effectively, it is learning off-policy, since tha state-action value update does not follow the current policy (behavior policy).

## TD control and Bellman equations
