---
layout: single
title:  "Notes for Generative AI learning, part 1"
date:   2024-09-01 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "Scratch notes when reading the O'Reilly book of 'Generative Deep Learning, 2nd Edition'"
header:
  teaser: assets/images/transformer_architecture.png
---

### Chapter 1 and 2

Mostly introductory to generative models and neural networks. Nothing very new to me.

### Chapter 3. Variational Autoencoders

The key idea is instead of mapping an input to a fixed point in the latent space,
we map it to a (standard normal) distribution in the latent space.

How to create an image from a latent vector? With `tensorflow/keras`, we can use
either the `Conv2DTranspose` layer or the `UpSampling2D` layer. The idea is the same:
we first increase the size of the 2D space, by either filling with zeros or repeating
the nearest pixel. With the enlarged 2D space, we then apply the convolutional operation
with stride of 1.

### Chapter 4, Generative Adversarial Networks

When training the generator, why do we set the label as "1", as if the generated image is real?
Note that in this phase, we assume we have a perfect discriminator which outputs the probability of the image being real.
Among the images generated by the generator, if by chance we create a truly real image, this will be
correctly identified by the discriminator as real (giving a high probability), therefore, we want to
reward such a situation by setting the label as "1" and use the binary cross-entropy as the loss function.
