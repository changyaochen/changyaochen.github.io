---
layout: single
title:  "Welford algorithm for updating variance"
date:   2019-02-06 12:00:01 -0600
published: false
tag: [algorithm]
excerpt: blah.
toc: true
header:
  teaser: /assets/images/welford_teaser.png
---
I came across this little algorithm by chance, and stunned by its simple form. It took me a whole page to derive it from end to end, it feels good that I can still do something like that!

The problem setting is pretty simple: we want to calculate the mean and variance of a numeric list, with length $$N$$, really nothing fancy, just straightup definitions:

$$
\begin{eqnarray*}
\mu_N &=& \frac{1}{N}\sum_{i=1}^{N} x_i~, \\
\sigma^2_N &=& \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu_N)^2~. 
\end{eqnarray*}
$$

Let me treat the variance as the population variance, hence the denominator of $$N$$, instead of $$N-1$$. In term of the speed complexicity, each of the caluclation is $$O(N)$$ since we have to touch every element of the list. 

Now a new element, $$x_{N+1}$$ is appended to the list, and we want to update the mean and variance of this new list with length $$N-1$$. Naturally, we just repeat the two $$O(N)$$ calculations, but can we do better, if we already know $$N, \mu_N, \sigma^2_N$$? For large $$N$$, it seems like a lot of effort with just one new element. Can we do better?

## The algorithm
Yes we can. Let's start from the new mean, $$\mu_{N+1}$$. This is rather straightward:

$$
\begin{eqnarray}
(N+1)\mu_{N+1} &=& \sum_{i=1}^{N+1} x_i\\
&=& \sum_{i=1}^{N} x_i + x_{N+1}\\
&=& N\mu_{N+1} + x_{N+1}\\
\mu_{N+1} &=& \mu_N + \frac{1}{N+1}(x_{N+1} - \mu_N)~. \tag{1}\label{new_mu}
\end{eqnarray}
$$

By doing so, there is no need to go through the whole list, bu rather just a few arithmetic operations, to update the mean. 

To get $$\sigma^2_{N+1}$$ is slightly more involved. First we recall the alternative form of variance as 

$$
\begin{eqnarray}
\sigma^2_N &=& \frac{1}{N} \sum_{i=1}^{N} x_i^2 - \mu_N^2~.
\end{eqnarray}
$$

From here, we can get two generic forms of:

$$
\begin{eqnarray}
N\sigma^2_N &=& \sum_{i=1}^{N} x_i^2 - N\mu_N^2~,\\
(N+1)\sigma^2_{N+1} &=& \sum_{i=1}^{N+1} x_i^2 - (N+1)\mu_{N+1}^2~\\
&=& \sum_{i=1}^{N} x_i^2 + x_{N+1}^2 - (N+1)\mu_{N+1}^2~. \tag{2}\label{new_sig}
\end{eqnarray}
$$

Note that, in Equation $$\eqref{new_sig}$$, there is **no need** to calculate the summation term, but rather retrieve its value from the known $$N, \mu_N, \sigma^2_N$$, as $$N(\sigma^2_N + \mu_N)$$, therefore, all three terms on the right-hand-side of Equation $$\eqref{new_sig}$$ can be calculated with $$O(1)$$, hence $$\sigma^2_{N+1}$$ can be obtained with $$O(1)$$ time too. 

Equations $$\eqref{new_mu}$$ and $$\eqref{new_sig}$$ are the essence of the [Welford algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm). However, to express Equation $$\eqref{new_sig}$$ in a more compact, recurrence-like format, it requires some paitence. Below I will just show the some key steps:

$$
\begin{eqnarray}
(N+1)\sigma^2_{N+1} &=& \sum_{i=1}^{N} x_i^2 + x_{N+1}^2 - (N+1)\mu_{N+1}^2~,\\
&=& \color{red}{N\sigma^2_N + N \mu_N} + x_{N+1}^2 - (N+1)\mu_{N+1}^2~,\\
&=& N\sigma^2_N + N \mu_N + x_{N+1}^2 
	- \color{red}{\frac{1}{N+1}(N\mu_{N} + x_{N+1})^2}\\
&=& N\sigma^2_N + N \mu_N + x_{N+1}^2 
	- \color{red}{\frac{1}{N+1}(N^2\mu_{N}^2 + 2\mu_{N}x_{N+1} + x_{N+1}^2)}.
\end{eqnarray}
$$

Multiplying both sides with $$N+1$$, and after some expansions, we then have:

$$
\begin{eqnarray}
(N+1)^2 \sigma_{N+1}^2 
&=& N(N+1) \sigma_{N}^2 + N(\mu_N^2 - 2\mu_{N}x_{N+1} + x_{N+1}^2) \\
&=& N(N+1) \sigma_{N}^2 + N(\mu_{N} - x_{N+1})^2.
\end{eqnarray}
$$

Now we can finally express $$\sigma_{N+1}^2$$ in a cleaner, recurrence relationship as:

$$
\begin{eqnarray}
\sigma_{N+1}^2 
&=& \sigma_{N}^2 + \frac
	{N(\mu_{N} - x_{N+1})^2 - (N+1)\sigma_N^2}
	{(N+1)^2}
\tag{3.1}\label{final_sig_1}
\end{eqnarray}
$$

Equation $$\eqref{final_sig_1}$$ is already rather compact, however, there is another equvalent expression for it, as we carry out some further arithmetic tricks on the second part of right hand side of Equation $$\eqref{final_sig_1}$$, as:

$$
\begin{eqnarray}
&~&\frac
	{N(\mu_{N} - x_{N+1})^2 - (N+1)\sigma_N^2}
	{(N+1)^2} \\ 
&=& 
\frac
	{(x_{N+1} - \mu_N)N(x_{N+1} - \mu_N) - (N+1)\sigma_N^2}
	{(N+1)^2}\\
&=& 
\frac
	{N(x_{N+1} - \mu_N)N\color{red}{(N+1)(\mu_{N+1} - \mu_N)} - (N+1)\sigma_N^2}
	{(N+1)^2}\\
&=& 
\frac
	{(x_{N+1} - \mu_N)N(\mu_{N+1} - \mu_N) - \sigma_N^2}
	{(N+1)} \\
&=& 
\frac
	{(x_{N+1} - \mu_N)\color{red}{(x_{N+1} - \mu_{N+1})} - \sigma_N^2}
	{(N+1)}.\\
\end{eqnarray}
$$

The transformations rely on different forms of Equation $$\eqref{new_mu}$$, and we have:

$$
\begin{eqnarray}
\sigma_{N+1}^2
&=& 
\sigma_N^2 + 
\frac
	{(x_{N+1} - \mu_N)(x_{N+1} - \mu_{N+1}) - \sigma_N^2}
	{(N+1)}. \tag{3.2}\label{final_sig_2}
\end{eqnarray}
$$

Both Equations $$\eqref{final_sig_1}$$ and $$\eqref{final_sig_2}$$ can be used equvalently.

## Numerical stability

Other than the $$O(N)$$ to $$O(1)$$ speed gain, the Welford algorithm provides an added benefit, which is the numerical stability. Since real numbers are represented using floating-point arithmetic with finite bits in computers, one has to lose some precision. In almost all cases, this is not a concern, however, if one subtracts a very small number from a very large number, or performs subtraction between two very close numbers, bad things can happen, due to this rounding issue. This is known as the [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance), or catastrophic cancellation.

How this catastrophic cancellation can affect the variance calculation?

## How about median?
