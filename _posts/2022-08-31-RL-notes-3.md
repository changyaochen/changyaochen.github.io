---
layout: single
title:  "Notes on Reinforcement Learning: GPI"
date:   2022-08-31 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---
In the previous post, we have defined the reinforcement learning (RL) as Markov Decision Process (MDP), and introduced the concept of reward, policy, and value functions. We established that the goal of RL is to identify a policy that maximizes the state-value functions, and ended with Bellman equation, which can be used to calculate the value functions.

To solve the Bellman equation (under a given policy), one can, in principle, use a linear system solver. However, this is usually infeasible, as size of the problem is too large (*e.g.*, identical to the total number of states). Instead, one usually solve this Dynamic programming problem iteratively.

With value functions, we can compare two different policies. However, it does not answer the question of: how to find the policy that attains the highest value function?

## Optimal value and policy

<p style="color:blue">We will limit the following discussion in the case of deterministic policy.</p>

If the state-value function under a given policy is higher than under *any other* policy, then this state-value function is called the optimal state-value (subscripted with $$*$$), and the corresponding policy is called optimal policy, $$\pi_*$$. Put it differently, under the optimal policy, the value function at each state is largest among all possible policies.
While the $$v_*(s)$$ is unique (as they are scalar), there can be multiple optimal policies.

If we already know the optimal state-value functions, $$v_*(s)$$, it is quite trivial to derive the optimal policy, as:

$$
\pi_*(a \mid s) = \text{argmax}_{a} \sum_{s', r}p(s', r \mid s, a)\big[r + \gamma v_*(s')\big]
$$

Basically, at each state $$s$$, we choose the action that maximizes the expected total future rewards. Along this line of logic, one can reason the form of the Bellman optimality equation as:

$$
\begin{align*}
v_{*}(s) &= {\color{red}{\max_{a}}} \sum_{s', r} p(s', r \mid s, a)\big[r + \gamma ~ {v_{*}(s')}\big] \\

q_{*}(s, a) &= \sum_{s', r} p(s', r | s, a)\big[r + \gamma ~ {\color{red}{\max_{a'}}}~{q_{*}(s', a')}\big]
\end{align*}
$$

If the state-value function for a given policy -- calculated by the generic Bellman equation -- equals the state-value function derived from the Bellman optimality equation (note that no policy is involved), then this policy is the optimal policy, $$\pi_*$$, and the state-value function is accordingly called $$v_*$$. Note that $$v_*$$ is unique, but there can be multiple $$\pi_*$$ lead to the same $$v_*$$.

## Nomenclature

|         Symbol/term | Definition                                                                                                                                                                                    |
| ------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|        $$v_{*}(s)$$ | The optimal state-value function.                                                                                                                                                             |
|     $$q_{*}(s, a)$$ | The optimal action-value function.                                                                                                                                                            |
| $$\pi_*(a \mid s)$$ | The optimal policy.                                                                                                                                                                           |
|   Policy evaluation | Also known as **prediction**. It is the task to determine the state-value function, $$v_\pi(s)$$, for a given policy $$\pi$$.                                                                 |
|  Policy improvement | Also known as **control**. It is the task to improve the policy, that is, to find the policy, $$\pi_*$$, that maximize the value functions (rewards). **Control is the ultimate goal of RL**. |

## Policy iteration

According to the structure of the optimal policy, it gives the hint how to find it, iteratively. We can start from a random policy, $$\pi_0$$, and use Dynamic Programming to calculate the state-value function, $$v_{\pi_0}(s)$$ (policy evaluation). Then at each state, we apply the $$\text{argmax}$$ rule, to arrive at a new policy $$\pi_1$$ (policy improvement).

<figure>
<center>
<a href="/assets/images/RL_policy_evaluation_control.png"><img style="width:100%;" src="/assets/images/RL_policy_evaluation_control.png"></a>
</center>
</figure>