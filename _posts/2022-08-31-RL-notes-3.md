---
layout: single
title:  "Notes on Reinforcement Learning: GPI"
date:   2022-08-31 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---

## Optimal policy

<p style="color:blue">We will limit the following discussion in the case of deterministic policy.</p>

If the state-value function under a given policy is higher than under any other policy, then this state-value function is called the optimal state-value (subscripted with $$*$$), and the corresponding policy is called optimal policy, $$\pi_*$$. Put it differently, under the optimal policy, the value function at each state is largest among all possible policies.
While the $$v_*(s)$$ is unique (as they are scalar), there can be multiple optimal policies.

If we already know the optimal state-value functions, $$v_*(s)$$, it is quite trivial to find the optimal policy, as:

$$
\pi_*(a \mid s) = \text{argmax}_{a} \sum_{s', r}p(s', r \mid s, a)\big[r + \gamma v_*(s')\big]
$$

Basically, at each state $$s$$, we choose the action that maximizes the expected total future rewards. Along this line of logic, one can reason the form of the Bellman optimality equation as:

$$
\begin{align*}
v_{*}(s) &= {\color{red}{\max_{a}}} \sum_{s', r} p(s', r \mid s, a)\big[r + \gamma ~ {v_{*}(s')}\big] \\

q_{*}(s, a) &= \sum_{s', r} p(s', r | s, a)\big[r + \gamma ~ {\color{red}{\max_{a'}}}~{q_{*}(s', a')}\big]
\end{align*}
$$

If the state-value function for a given policy -- calculated by the generic Bellman equation -- equals the state-value function derived from the Bellman optimality equation (note that no policy is involved), then this policy is the optimal policy, $$\pi_*$$, and the state-value function is accordingly called $$v_*$$. Note that $$v_*$$ is unique, but there can be multiple $$\pi_*$$ lead to the same $$v_*$$.

## General policy iteration
blah
<figure>
<center>
<a href="/assets/images/RL_policy_evaluation_control.png"><img style="width:100%;" src="/assets/images/RL_policy_evaluation_control.png"></a>
</center>
</figure>