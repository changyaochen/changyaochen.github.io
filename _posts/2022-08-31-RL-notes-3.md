---
layout: single
title:  "Notes on Reinforcement Learning: GPI"
date:   2022-08-31 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---
In the previous post, we have defined the reinforcement learning (RL) as Markov Decision Process (MDP), and introduced the concept of reward, policy, and value functions. We established that the goal of RL is to identify a policy that maximizes the state-value functions, and ended with Bellman equation, which can be used to calculate the value functions.

To solve the Bellman equation (under a given policy), one can, in principle, use a solver based on linear algebra. However, this is usually infeasible, as size of the problem is too large (*e.g.*, identical to the total number of states). Instead, one usually solve this Dynamic programming problem iteratively.

With value functions, we can compare two different policies. However, it does not answer the question of: how to find the policy that attains the highest value function?

## Optimal value and policy

<p style="color:blue">We will limit the following discussion in the case of deterministic policy.</p>

If the state-value function under a given policy is higher than under any other policy, then this state-value function is called the optimal state-value (subscripted with $$*$$), and the corresponding policy is called optimal policy, $$\pi_*$$. Put it differently, under the optimal policy, the value function at each state is largest among all possible policies.
While the $$v_*(s)$$ is unique (as they are scalar), there can be multiple optimal policies.

If we already know the optimal state-value functions, $$v_*(s)$$, it is quite trivial to find the optimal policy, as:

$$
\pi_*(a \mid s) = \text{argmax}_{a} \sum_{s', r}p(s', r \mid s, a)\big[r + \gamma v_*(s')\big]
$$

Basically, at each state $$s$$, we choose the action that maximizes the expected total future rewards. Along this line of logic, one can reason the form of the Bellman optimality equation as:

$$
\begin{align*}
v_{*}(s) &= {\color{red}{\max_{a}}} \sum_{s', r} p(s', r \mid s, a)\big[r + \gamma ~ {v_{*}(s')}\big] \\

q_{*}(s, a) &= \sum_{s', r} p(s', r | s, a)\big[r + \gamma ~ {\color{red}{\max_{a'}}}~{q_{*}(s', a')}\big]
\end{align*}
$$

If the state-value function for a given policy -- calculated by the generic Bellman equation -- equals the state-value function derived from the Bellman optimality equation (note that no policy is involved), then this policy is the optimal policy, $$\pi_*$$, and the state-value function is accordingly called $$v_*$$. Note that $$v_*$$ is unique, but there can be multiple $$\pi_*$$ lead to the same $$v_*$$.

## General policy iteration
blah
<figure>
<center>
<a href="/assets/images/RL_policy_evaluation_control.png"><img style="width:100%;" src="/assets/images/RL_policy_evaluation_control.png"></a>
</center>
</figure>