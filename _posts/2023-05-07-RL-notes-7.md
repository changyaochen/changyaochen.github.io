---
layout: single
title:  "Notes on Reinforcement Learning: Planning"
date:   2023-05-07 12:00:00 -0600
published: true
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---
In the previous posts of this series, we have discussed how the agent can learn what is the best action to take in a given state (_i.e._, the policy), but interacting with the environment **without** a model, _i.e._, in the model-free setting. These approaches include Monte Carlo method, and Temporal Difference (TD) method, _e.g._, Q-learning. Recall that in the earlier posts, we discussed algorithms in the model-based setting, where the transition probability $$p(s', r \mid s, a)$$ is known, and the agent can plan ahead to evaluate the behavior policy and improve the policy towards the optimal policy (Dynamic Programming with bootstrapped values).

## Combining Model-Free and Model-Based Methods

Can we combine the model-free and model-based methods? Yes, we can. In fact, this is a very common approach in practice. The idea is to use the model-free method to interact with the environment (sampling) in order to learn the underlying model, and use the model-based method to improve the policy (_e.g._, calculate the state-action value functions). The latter is known as the (state-space) planning.

### Dyna-Q
It is not hard to see how one can intermix the sampling (or acting) and planning. For example, we can sample from the environment for one step, then update the state-action value function (via either SARSA or Q-learning). Since we just learned something from the environment, we should update our understanding of the environment, _i.e._, the model. This can be done simply by updating tabular information of the next state and reward. With such a table that keeps track of all the $$(S, A)$$ we have visited and the outcome (next state and the reward), we can run some simulation, as if this tabular model **is** the environment, and let the agent interact with it to keep updating the state-action value function.

Below is the pseudo-code for Dyna-Q, which is a combination of Q-learning and planning.

<figure>
<center>
<a href="/assets/images/DynaQ_pseudo.png"><img style="width:100%;" src="/assets/images/DynaQ_pseudo.png"></a>
</center>
</figure>

As you can see, it is largely identical to the Q-learning algorithm, except that we add a model update step (e), and a planning step (f) in which the agent interacts with the model to continue updating the state-action value function.

### Planning smarter

In the Dyna-Q algorithm shown above, each of the planning step starts with a random state, and apply the same state-action value function udpate rule. There can be a few issues with that.

First, the uniformly random state may not be the most efficient way to plan: after all, we would like to see changes in $$Q(S, A)$$, therefore, we would like to focus on states whose q-values are recently changed, and sample the state-action pairs that leading to them. To achieve this, we can use a priority queue to keep track of the states that we have visited, and the priority is the magnitude of the change in $$Q(S, A)$$. We can then sample from this priority queue to in our planning step. This is known as the Prioritized Sweeping algorithm.

Second, there is only one (or a few) acting step (sampling from the real environment) between the (possibly many) planning steps. During each planning step, we run many simulations in which we are disconnected from the real environment and solely rely on the assumption the model is correct. However, the environment might have changed, hence, we need to encourage the agent to visit outdated states (states that we have not visited for a long time) more often (the Dyna-Q+ algorithm).

## Putting things together
For tabular methods.
