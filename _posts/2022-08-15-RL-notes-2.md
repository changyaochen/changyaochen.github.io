---
layout: single
title:  "Notes on Reinforcement Learning: MDP"
date:   2022-08-15 12:00:00 -0600
published: true
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---

In the previous note, we have highlighted the difference between Bandit and Reinforcement Learning (RL). In the former, the reward is immediate, and we want to identify the best action that maximizes this reward. In the latter, the reward is usually delayed, and the best action depends on the state where the agent is in. Moreover, the action will impact the future reward, so any action will have long term consequences.

This process can be modeled as [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process), MDP. The central assumption of MDP is the memoryless [Markov property](https://en.wikipedia.org/wiki/Markov_property): given the current state $$s$$ and action $$a$$, the state transition probability, and the reward probability, are both independent of all previous states and actions. If both the state space and action space are finite, then it is called a finite MDP.

The diagram below illustrates the close-loop nature of a RL system: the agent interacts with the environment by taking actions, receiving rewards, and moving to a different state.

<figure>
<center>
<a href="/assets/images/reinforcement-learning-fig.jpeg"><img style="width:100%;" src="/assets/images/reinforcement-learning-fig.jpeg"></a>
</center>
</figure>

## Nomenclature
We have introduced some nomenclature in the previous post, mostly in the context of bandit. Here we will add the common terminologies used in the RL context. Similarly, the subscript $$t$$ denotes the time step.

|                 Symbol | Definition                                                                                                                        |
| ---------------------: | --------------------------------------------------------------------------------------------------------------------------------- |
|       $$\pi(a\mid s)$$ | An policy, which in general is probability distribution. It prescribes the probability of taking action $$a$$ in the state $$s$$. |
|          $$v_\pi (s)$$ | The state-value function of state $$s$$, under the policy $$\pi$$.                                                                |
|       $$q_\pi (s, a)$$ | The action-value function of state-action pair $$(s, a)$$, under the policy $$\pi$$.                                              |
| $$p(s', r \mid s, a)$$ | The joint probability distribution of transition and reward.                                                                      |
|                $$G_t$$ | Total rewards from time step $$t$$, _i.e._, including future rewards.                                                             |
|             $$\gamma$$ | Discount rate of future rewards.                                                                                                  |

## Policy
A policy $$\pi$$ defines the action, $$a$$, the agent will tak, when in a given state, $$s$$. The policy can be either deterministic (when seeing a 4-way roundabout, alway turn left), or probabilistic (50% chance turn left, 15% chance go straight, 15% chance turn right, 20% chance turn back). After each action, the agent will receive a reward, $$r$$, from the environment. This reward is usually stochastic as well.

The goal of RL is to find a policy that maximizes the total reward **in the long term**.

## Value functions
For a given policy, one can calculate the value functions: it estimates the future return under this policy. There are two types of value functions: state-value function, and action-value function. They are defined as:

$$
\begin{align}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t | S_t = s]\\
q_{\pi}(s, a) &= \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{align}
$$

Note here $$G_t$$ is the total reward from time step $$t$$, that is, it is the sum of $$R_{t + 1}, R_{t + 2}, ...$$, until the episode stops (*e.g.*, winning a chess game).

To make it tractable, the concept of *discounting* is introduced. In this setting, the future rewards are discounted, as $$G_t$$ is now defined as:

$$
G_t := R_{t + 1} + \gamma R_{t + 2} + \gamma^2 R_{t + 3} + ... = R_{t + 1} + \gamma G_{t + 1}
$$

Obviously the discount rate $$\gamma \in [0, 1)$$. Specifically, if $$\gamma = 0$$, then we only consider the immediate reward, hence turning into the bandit case.

Intuitively, the value function measures *how good* it is to be in a state (or taking a given action in that state). For example, in a 2-dimensional grid world, if the goal is to reach a certain cell, then cells (states) close to the goal cell would have a higher state-value. Also, the value functions depend on the policy $$\pi$$: in this grid world example, if the policy prescribes the agent to always move left, then the state left to the goal-state will have a state-value function smaller than that of a policy to always move right.

The goal of RL can be boiled down to:
1. How to calculate the value functions for a given policy.
2. How to find the policy that has the highest value-functions.

## Dynamic programming and Bellman equation
For a given policy $$\pi$$, if the transition probability $$p(s', r \mid s, a)$$ is known, then the a single state-value function can be described by other state-value functions as:

$$
\begin{align*}
v_{\pi}(s) &= \sum_{a} \pi(a | s) \sum_{s', r} p(s', r \mid s, a)\big[r + \gamma ~ \color{red}{v_{\pi}(s')}\big] \\
  &\text{for all } s \in \mathcal{S}
\end{align*}
$$

Intuitively, it states that the value of the given state, $$v_\pi(s)$$, is consists of immediate reward, $$r$$ and discounted state-values of possible successive states, $$v_\pi(s')$$. It also depends on where does the agent do -- controlled by $$\pi$$, and what is the future state -- controlled by $$p(s', r \mid s, a)$$.

Note that, the state-value function of $$v_\pi(s)$$ is expressed recursively (its own value on the right-hand side), as if we know all state-value functions $$v_\pi(s')$$. This is the hallmark of [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming). In this RL setting, this is known as the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation).

For a given RL problem, there are $$\mid \mathcal{S} \mid$$ state-value functions, one for each state. Therefore, it is a set of linear equations, and in principle, they can be solved analytically. However, it is usually infeasible to do so in practice, since the number of states is too large. Later we will introduce iterative algorithms to solve for the value functions.

> The Bellman equation answers the question of "how to calculate the value functions for a given policy"

Similarly, one can derive the Bellman equation for the action-value functions as:

$$
\begin{align}
q_{\pi}(s, a) &= \sum_{s', r} p(s', r | s, a)\big[r + \gamma ~ \sum_{a'} \pi(a' | s') \color{red}{q_{\pi}(s', a')}\big]\\
  &\text{for all }s \in \mathcal{S}, a \in \mathcal{A}
\end{align}
$$

There are $$\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid$$ action-value functions.

## Optimal policy

<p style="color:blue">We will limit the following discussion in the case of deterministic policy.</p>

If the state-value function under a given policy is higher than under any other policy, then this state-value function is called the optimal state-value (subscripted with $$*$$), and the corresponding policy is called optimal policy, $$\pi_*$$. Put it differently, under the optimal policy, the value function at each state is largest among all possible policies.
While the $$v_*(s)$$ is unique (as they are scalar), there can be multiple optimal policies.

If we already know the optimal state-value functions, $$v_*(s)$$, it is quite trivial to find the optimal policy, as:

$$
\pi_*(a \mid s) = \text{argmax}_{a} \sum_{s', r}p(s', r \mid s, a)\big[r + \gamma v_*(s')\big]
$$

Basically, at each state $$s$$, we choose the action that maximizes the expected total future rewards. Along this line of logic, one can reason the form of the Bellman optimality equation as:

$$
\begin{align*}
v_{*}(s) &= {\color{red}{\max_{a}}} \sum_{s', r} p(s', r \mid s, a)\big[r + \gamma ~ {v_{*}(s')}\big] \\

q_{*}(s, a) &= \sum_{s', r} p(s', r | s, a)\big[r + \gamma ~ {\color{red}{\max_{a'}}}~{q_{*}(s', a')}\big]
\end{align*}
$$

If the state-value function for a given policy -- calculated by the generic Bellman equation -- equals the state-value function derived from the Bellman optimality equation (note that no policy is involved), then this policy is the optimal policy, $$\pi_*$$, and the state-value function is accordingly called $$v_*$$. Note that $$v_*$$ is unique, but there can be multiple $$\pi_*$$ lead to the same $$v_*$$.

To answer the second question we posted above, namely, how to calculate $$v_*$$ and $$\pi_*$$, we will leave it to the next post.
