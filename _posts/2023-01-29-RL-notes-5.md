---
layout: single
title:  "Notes on Reinforcement Learning: Temporal Difference learning"
date:   2023-01-20 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---

TD learning: combines DP and MC.
Like DP, it bootstraps (assuming information we don't accurately know).
Like MC, it learns directly from the experience by sampling.

MC is only applicable to episodic situation, but TD doesn't have this constraint.
MC doesn't have the Markovian constraint, but TD and DP have, as they both bootstrap.

TD target

TD error: dopamine

$$
V(s_t) \leftarrow V(s_t) + \alpha \overbrace{(\underbrace{[r_t + \gamma V(s_{t + 1})]}_{\text{TD target}} - V(s_t))}^{\text{TD error}}
$$

Comparing between DP and MC. To DP, no need for model. To MC, no need to wait till the end.

## A simple example

In this example, the agent starts from state 3, and always takes a 50/50 action between moving left or right. The state 0 and 6 are terminal states, and moving from state 5 to 6 gives the agent a reward of +1. As such, this is an episodic task. We will use 3 different approaches: Dynamic Programming (DP), Monte Carlo (MC), and Temporal Difference to solve for the state values.

<figure>
<center>
<a href="/assets/images/dp_mc_td_example.png"><img style="width:100%;" src="/assets/images/dp_mc_td_example.png"></a>
</center>
</figure>

$$
\begin{eqnarray}
P = p(s' \mid  ) = 0.5 \times
  \begin{bmatrix}
  0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  1 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 & 1 \\
  0 & 0 & 0 & 0 & 0 & 0 & 0
  \end{bmatrix} \\

R =
  \begin{bmatrix}
  0 & 0 & 0 & 0 & 0 & 0.5 & 0
  \end{bmatrix}

\end{eqnarray}

$$
