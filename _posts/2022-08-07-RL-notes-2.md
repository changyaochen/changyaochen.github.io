---
layout: single
title:  "Notes on Reinforcement Learning: MDP"
date:   2022-08-08 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---

In the previous note, we have highlighted the difference between Bandit and Reinforcement Learning (RL). In the former, the reward is immediate, and we want to identify the best action that maximize this reward. In the latter, the reward is usually delayed, and the best action depends on the state where the agent is in. Moreover, the action will impact the future reward, so any action will have long term consequence.

This process can be modeled as [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process), MDP. The central assumption of MDP is the memoryless [Markov property](https://en.wikipedia.org/wiki/Markov_property): given the current state $$s$$ and action $$a$$, the state transition probability, and the reward probability, are both independent of all previous states and actions. If both the state space and action space are finite, then it is called a finite MDP.

The diagram below illustrates the close-loop nature of a RL system: the agent interacts with the environment by taking actions, receiving rewards, and moving to a different state.

<figure>
<center>
<a href="/assets/images/reinforcement-learning-fig.jpeg"><img style="width:100%;" src="/assets/images/reinforcement-learning-fig.jpeg"></a>
</center>
</figure>

## Nomenclature
We have introduced some nomenclature in the previous post, mostly in the context of bandit. Here we will add the common terminologies used in the RL context. Similarly, the subscript $$t$$ denotes the time step.

|                 Symbol | Definition                                                                                                                        |
| ---------------------: | --------------------------------------------------------------------------------------------------------------------------------- |
|       $$\pi(a\mid s)$$ | An policy, which in general is probability distribution. It prescribes the probability of taking action $$a$$ in the state $$s$$. |
|          $$v_\pi (s)$$ | The state-value function of state $$s$$, under the policy $$\pi$$.                                                                |
|       $$q_\pi (s, a)$$ | The action-value function of state-action pair $$(s, a)$$, under the policy $$\pi$$.                                              |
| $$p(s', r \mid s, a)$$ | The joint probability distribution of transition and reward.                                                                      |
|                $$G_t$$ | Total rewards from time step $$t$$, _i.e._, including future rewards.                                                             |
|             $$\gamma$$ | Discount rate of future rewards.                                                                                                  |

## Policy
A policy $$\pi$$ defines the action, $$a$$, the agent will tak, when in a given state, $$s$$. The policy can be either deterministic (when seeing a 4-way roundabout, alway turn left), or probabilistic (50% chance turn left, 15% chance go straight, 15% chance turn right, 20% chance turn back). After each action, the agent will receive a reward, $$r$$, from the environment. This reward is usually stochastic as well.

The goal of RL, is to find the policy that maximize the total reward **in the long term**.

## Value functions
For a given policy, one can calculate the value functions: it estimates the future return under this policy. There are two types of value functions: state-value function, and action-value function. They are defined as:

$$
\begin{align}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t | S_t = s]\\
q_{\pi}(s, a) &= \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]
\end{align}
$$

Note here $$G_t$$ is the total reward from time step $$t$$, that is, it is the sum of $$R_{t + 1}, R_{t + 2}, ...$$, until the episode stops (*e.g.*, winning a chess game).

To make it tractable, the concept of *discounting* is introduced. In this setting, the future rewards are discounted, as $$G_t$$ is now defined as:

$$
G_t := R_{t + 1} + \gamma R_{t + 2} + \gamma^2 R_{t + 3} + ... = R_{t + 1} + \gamma G_{t + 1}
$$

Obviously the discount rate $$\gamma \in [0, 1)$$. Specifically, if $$\gamma = 0$$, then we only consider the immediate reward, hence turning into the bandit case.

Intuitively, the value function quantifies *how good* to be in a state (or taking a given action in that state): the higher, the better. For example, in a 2-dimensional grid world, if the goal is to reach a certain cell, then cells (states) close to the goal cell would have a higher state-value. Also, the value functions depend on the policy $$\pi$$: in the grid world example, if the policy prescribe the agent always move left, then the state left to the goal-state will have a state-value function smaller than that of a policy always move right.

<figure>
<center>
<a href="/assets/images/RL_policy_evaluation_control.png"><img style="width:100%;" src="/assets/images/RL_policy_evaluation_control.png"></a>
</center>
</figure>