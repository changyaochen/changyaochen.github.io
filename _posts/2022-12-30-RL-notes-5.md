---
layout: single
title:  "Notes on Reinforcement Learning: Temporal D  ifference learning"
date:   2023-01-07 12:00:00 -0600
published: false
tag: [machine learning]
toc: true
toc_sticky: true
excerpt: "A collections of notes of Reinforcement Learning,
as I am going through the Coursera specialization: Fundamentals of Reinforcement Learning.
Hopefully this will be useful for future self."
header:
  teaser: /assets/images/reinforcement-learning-fig.jpeg
---

TD learning: combines DP and MC. Like DP, it bootstrap, like MC, it learns directly from the experience by sampling.

MC is only applicable to episodic situation,

TD target

TD error: dopamine

Comparing between DP and MC. To DP, no need for model. To MC, no need to wait till the end.

## A simple example

<figure>
<center>
<a href="/assets/images/dp_mc_td_example.png"><img style="width:100%;" src="/assets/images/dp_mc_td_example.png"></a>
</center>
</figure>

$$
\begin{eqnarray}
p(s' \mid s, \leftarrow) =
  \begin{bmatrix}
  1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 & 0
  \end{bmatrix} \\

p(s' \mid s, \rightarrow) =
  \begin{bmatrix}
  0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1
  \end{bmatrix}

\end{eqnarray}

$$
